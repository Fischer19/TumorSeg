{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TwoPathConv (\n",
      "  (upper_layer1): Sequential (\n",
      "    (0): Conv2d(4, 64, kernel_size=(7, 7), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(4, 4), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (upper_layer2): Sequential (\n",
      "    (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "    (2): MaxPool2d (size=(2, 2), stride=(1, 1), dilation=(1, 1))\n",
      "  )\n",
      "  (under_layer1): Sequential (\n",
      "    (0): Conv2d(4, 160, kernel_size=(13, 13), stride=(1, 1))\n",
      "    (1): ReLU ()\n",
      "  )\n",
      "  (final_layer): Conv2d(224, 5, kernel_size=(21, 21), stride=(1, 1))\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoPathConv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoPathConv, self).__init__()\n",
    "        self.upper_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,64,7),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((4,4),stride = 1)\n",
    "        )\n",
    "        self.upper_layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64,64,3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d((2,2),stride = 1)\n",
    "        )\n",
    "        self.under_layer1 = nn.Sequential(\n",
    "            nn.Conv2d(4,160,13),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final_layer = nn.Conv2d(224,5,21)\n",
    "        #self.upper_conv1 = nn.Conv2d(4,64,7)\n",
    "        #self.upper_conv2 = nn.Conv2d(64,64,3)\n",
    "        #self.under_conv1 = nn.Conv2d(4,160,13)\n",
    "        #self.final_conv = nn.Conv2d(224,5,21)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        upper_x = self.upper_layer2(self.upper_layer1(x))\n",
    "        under_x = self.under_layer1(x)\n",
    "        #upper_x = F.max_pool2d(F.relu(self.upper_conv1(x)), (4, 4),stride = 1)\n",
    "        #upper_x = F.max_pool2d(F.relu(self.upper_conv2(upper_x)), (2, 2), stride = 1)\n",
    "        #under_x = F.relu(self.under_conv1(x))\n",
    "        final_x = torch.cat((under_x, upper_x), 1)\n",
    "        out = F.softmax(self.final_layer(final_x))\n",
    "        return out\n",
    "        \n",
    "net = TwoPathConv()\n",
    "print(net)\n",
    "\n",
    "x = Variable(torch.randn(1,4,33,33), requires_grad = True)\n",
    "y_pred = net.forward(x)\n",
    "#y_pred = y_pred.data.resize_(1,5)\n",
    "#y_pred = Variable(y_pred,requires_grad = True)\n",
    "#print(y_pred.size())\n",
    "#print(y_pred.view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "Variable containing:\n",
      " 0.9048\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "label = Variable(torch.LongTensor([3]), requires_grad=False)\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "for i in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    y_pred = net.forward(x)\n",
    "    y_pred = y_pred.resize(1,5)\n",
    "    loss = F.nll_loss(F.log_softmax(y_pred), label)\n",
    "    loss.backward()\n",
    "    #print(loss)\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 678.2203979492188)\n",
      "(1, 660.6570434570312)\n",
      "(2, 643.557373046875)\n",
      "(3, 626.8915405273438)\n",
      "(4, 610.68359375)\n",
      "(5, 594.9915771484375)\n",
      "(6, 579.7462158203125)\n",
      "(7, 564.9285278320312)\n",
      "(8, 550.4617919921875)\n",
      "(9, 536.4539184570312)\n",
      "(10, 522.885986328125)\n",
      "(11, 509.7567443847656)\n",
      "(12, 497.0480041503906)\n",
      "(13, 484.7501525878906)\n",
      "(14, 472.7842102050781)\n",
      "(15, 461.11737060546875)\n",
      "(16, 449.7706604003906)\n",
      "(17, 438.8199157714844)\n",
      "(18, 428.1475830078125)\n",
      "(19, 417.7566223144531)\n",
      "(20, 407.73785400390625)\n",
      "(21, 397.9593505859375)\n",
      "(22, 388.41912841796875)\n",
      "(23, 379.1401062011719)\n",
      "(24, 370.1069030761719)\n",
      "(25, 361.2682189941406)\n",
      "(26, 352.64422607421875)\n",
      "(27, 344.20477294921875)\n",
      "(28, 335.9613037109375)\n",
      "(29, 327.9267883300781)\n",
      "(30, 320.1400146484375)\n",
      "(31, 312.5726013183594)\n",
      "(32, 305.17242431640625)\n",
      "(33, 297.9583435058594)\n",
      "(34, 290.9154968261719)\n",
      "(35, 284.0159606933594)\n",
      "(36, 277.3037414550781)\n",
      "(37, 270.73114013671875)\n",
      "(38, 264.2970275878906)\n",
      "(39, 258.0490417480469)\n",
      "(40, 251.9435577392578)\n",
      "(41, 245.95468139648438)\n",
      "(42, 240.09048461914062)\n",
      "(43, 234.35781860351562)\n",
      "(44, 228.716796875)\n",
      "(45, 223.2110595703125)\n",
      "(46, 217.8192596435547)\n",
      "(47, 212.54299926757812)\n",
      "(48, 207.3648223876953)\n",
      "(49, 202.30078125)\n",
      "(50, 197.3597412109375)\n",
      "(51, 192.54586791992188)\n",
      "(52, 187.86155700683594)\n",
      "(53, 183.2745361328125)\n",
      "(54, 178.7635955810547)\n",
      "(55, 174.3333282470703)\n",
      "(56, 169.99424743652344)\n",
      "(57, 165.76002502441406)\n",
      "(58, 161.61517333984375)\n",
      "(59, 157.5563201904297)\n",
      "(60, 153.57740783691406)\n",
      "(61, 149.67518615722656)\n",
      "(62, 145.86090087890625)\n",
      "(63, 142.1291046142578)\n",
      "(64, 138.4845428466797)\n",
      "(65, 134.9213104248047)\n",
      "(66, 131.4197540283203)\n",
      "(67, 128.0012969970703)\n",
      "(68, 124.65477752685547)\n",
      "(69, 121.37601470947266)\n",
      "(70, 118.18017578125)\n",
      "(71, 115.05164337158203)\n",
      "(72, 111.97895812988281)\n",
      "(73, 108.96920013427734)\n",
      "(74, 106.02436065673828)\n",
      "(75, 103.13705444335938)\n",
      "(76, 100.30839538574219)\n",
      "(77, 97.5310287475586)\n",
      "(78, 94.8041000366211)\n",
      "(79, 92.12954711914062)\n",
      "(80, 89.51017761230469)\n",
      "(81, 86.94825744628906)\n",
      "(82, 84.44791412353516)\n",
      "(83, 81.99658966064453)\n",
      "(84, 79.59574890136719)\n",
      "(85, 77.24943542480469)\n",
      "(86, 74.95196533203125)\n",
      "(87, 72.7083511352539)\n",
      "(88, 70.51695251464844)\n",
      "(89, 68.37567138671875)\n",
      "(90, 66.28582000732422)\n",
      "(91, 64.24639892578125)\n",
      "(92, 62.251564025878906)\n",
      "(93, 60.30514144897461)\n",
      "(94, 58.406341552734375)\n",
      "(95, 56.55643081665039)\n",
      "(96, 54.752498626708984)\n",
      "(97, 52.992149353027344)\n",
      "(98, 51.27730178833008)\n",
      "(99, 49.609825134277344)\n",
      "(100, 47.98368835449219)\n",
      "(101, 46.402488708496094)\n",
      "(102, 44.861141204833984)\n",
      "(103, 43.36181640625)\n",
      "(104, 41.90278625488281)\n",
      "(105, 40.486629486083984)\n",
      "(106, 39.10954284667969)\n",
      "(107, 37.77277374267578)\n",
      "(108, 36.47364807128906)\n",
      "(109, 35.20954513549805)\n",
      "(110, 33.983558654785156)\n",
      "(111, 32.793766021728516)\n",
      "(112, 31.6402587890625)\n",
      "(113, 30.518381118774414)\n",
      "(114, 29.430728912353516)\n",
      "(115, 28.376611709594727)\n",
      "(116, 27.35264015197754)\n",
      "(117, 26.359899520874023)\n",
      "(118, 25.399301528930664)\n",
      "(119, 24.46742057800293)\n",
      "(120, 23.564529418945312)\n",
      "(121, 22.690011978149414)\n",
      "(122, 21.843738555908203)\n",
      "(123, 21.023014068603516)\n",
      "(124, 20.230871200561523)\n",
      "(125, 19.462675094604492)\n",
      "(126, 18.719762802124023)\n",
      "(127, 18.00174331665039)\n",
      "(128, 17.30771255493164)\n",
      "(129, 16.636856079101562)\n",
      "(130, 15.988525390625)\n",
      "(131, 15.361790657043457)\n",
      "(132, 14.757058143615723)\n",
      "(133, 14.173465728759766)\n",
      "(134, 13.608807563781738)\n",
      "(135, 13.064751625061035)\n",
      "(136, 12.540448188781738)\n",
      "(137, 12.03474235534668)\n",
      "(138, 11.546308517456055)\n",
      "(139, 11.075589179992676)\n",
      "(140, 10.621145248413086)\n",
      "(141, 10.182419776916504)\n",
      "(142, 9.758916854858398)\n",
      "(143, 9.350809097290039)\n",
      "(144, 8.957520484924316)\n",
      "(145, 8.578104972839355)\n",
      "(146, 8.213187217712402)\n",
      "(147, 7.861166000366211)\n",
      "(148, 7.522841453552246)\n",
      "(149, 7.197618007659912)\n",
      "(150, 6.884583950042725)\n",
      "(151, 6.583857536315918)\n",
      "(152, 6.294917106628418)\n",
      "(153, 6.017606258392334)\n",
      "(154, 5.751016616821289)\n",
      "(155, 5.495144844055176)\n",
      "(156, 5.249444961547852)\n",
      "(157, 5.0137786865234375)\n",
      "(158, 4.788151264190674)\n",
      "(159, 4.571361064910889)\n",
      "(160, 4.363542556762695)\n",
      "(161, 4.164396286010742)\n",
      "(162, 3.973623514175415)\n",
      "(163, 3.7906837463378906)\n",
      "(164, 3.615384101867676)\n",
      "(165, 3.4476540088653564)\n",
      "(166, 3.2871158123016357)\n",
      "(167, 3.133251428604126)\n",
      "(168, 2.9861278533935547)\n",
      "(169, 2.8454275131225586)\n",
      "(170, 2.710803508758545)\n",
      "(171, 2.582052707672119)\n",
      "(172, 2.459052324295044)\n",
      "(173, 2.3413398265838623)\n",
      "(174, 2.2290055751800537)\n",
      "(175, 2.1215896606445312)\n",
      "(176, 2.0189669132232666)\n",
      "(177, 1.9209210872650146)\n",
      "(178, 1.827345371246338)\n",
      "(179, 1.7379668951034546)\n",
      "(180, 1.652741551399231)\n",
      "(181, 1.5713627338409424)\n",
      "(182, 1.4937324523925781)\n",
      "(183, 1.4196784496307373)\n",
      "(184, 1.3490582704544067)\n",
      "(185, 1.2817341089248657)\n",
      "(186, 1.2174124717712402)\n",
      "(187, 1.1561269760131836)\n",
      "(188, 1.0976637601852417)\n",
      "(189, 1.0420421361923218)\n",
      "(190, 0.9890832901000977)\n",
      "(191, 0.938571035861969)\n",
      "(192, 0.8904346227645874)\n",
      "(193, 0.8446577191352844)\n",
      "(194, 0.8010278344154358)\n",
      "(195, 0.7595461010932922)\n",
      "(196, 0.7200734615325928)\n",
      "(197, 0.6825352311134338)\n",
      "(198, 0.6468526124954224)\n",
      "(199, 0.612879753112793)\n",
      "(200, 0.580618143081665)\n",
      "(201, 0.5499904155731201)\n",
      "(202, 0.5208033919334412)\n",
      "(203, 0.49310413002967834)\n",
      "(204, 0.46680667996406555)\n",
      "(205, 0.44183245301246643)\n",
      "(206, 0.41811978816986084)\n",
      "(207, 0.3955944776535034)\n",
      "(208, 0.37424060702323914)\n",
      "(209, 0.3539341688156128)\n",
      "(210, 0.33470648527145386)\n",
      "(211, 0.31647515296936035)\n",
      "(212, 0.2991947531700134)\n",
      "(213, 0.28281623125076294)\n",
      "(214, 0.2673165500164032)\n",
      "(215, 0.25260016322135925)\n",
      "(216, 0.2386726289987564)\n",
      "(217, 0.22548265755176544)\n",
      "(218, 0.2129896581172943)\n",
      "(219, 0.2011551856994629)\n",
      "(220, 0.1899610012769699)\n",
      "(221, 0.17935311794281006)\n",
      "(222, 0.16931597888469696)\n",
      "(223, 0.15983662009239197)\n",
      "(224, 0.15085469186306)\n",
      "(225, 0.1423603594303131)\n",
      "(226, 0.13432331383228302)\n",
      "(227, 0.1267179250717163)\n",
      "(228, 0.11952735483646393)\n",
      "(229, 0.11272837966680527)\n",
      "(230, 0.10630404949188232)\n",
      "(231, 0.1002245545387268)\n",
      "(232, 0.09448452293872833)\n",
      "(233, 0.08906153589487076)\n",
      "(234, 0.0839347094297409)\n",
      "(235, 0.07909319549798965)\n",
      "(236, 0.07451865077018738)\n",
      "(237, 0.07019510120153427)\n",
      "(238, 0.06611675024032593)\n",
      "(239, 0.062269773334264755)\n",
      "(240, 0.05863077566027641)\n",
      "(241, 0.055198702961206436)\n",
      "(242, 0.05196226015686989)\n",
      "(243, 0.048906486481428146)\n",
      "(244, 0.04602394998073578)\n",
      "(245, 0.04330373927950859)\n",
      "(246, 0.04073919355869293)\n",
      "(247, 0.03832069784402847)\n",
      "(248, 0.03603881224989891)\n",
      "(249, 0.0338878370821476)\n",
      "(250, 0.031860314309597015)\n",
      "(251, 0.02995017170906067)\n",
      "(252, 0.028151843696832657)\n",
      "(253, 0.02645518258213997)\n",
      "(254, 0.024857616052031517)\n",
      "(255, 0.02335270307958126)\n",
      "(256, 0.02193554863333702)\n",
      "(257, 0.020601173862814903)\n",
      "(258, 0.019347120076417923)\n",
      "(259, 0.01816302351653576)\n",
      "(260, 0.01705075614154339)\n",
      "(261, 0.01600291207432747)\n",
      "(262, 0.015017535537481308)\n",
      "(263, 0.01409101951867342)\n",
      "(264, 0.013219757005572319)\n",
      "(265, 0.012400057166814804)\n",
      "(266, 0.011629928834736347)\n",
      "(267, 0.010908598080277443)\n",
      "(268, 0.010230880230665207)\n",
      "(269, 0.009593828581273556)\n",
      "(270, 0.00899553019553423)\n",
      "(271, 0.008431520313024521)\n",
      "(272, 0.007902431301772594)\n",
      "(273, 0.007404666393995285)\n",
      "(274, 0.006937149912118912)\n",
      "(275, 0.006497954484075308)\n",
      "(276, 0.006085648667067289)\n",
      "(277, 0.005698328372091055)\n",
      "(278, 0.005334546323865652)\n",
      "(279, 0.00499314907938242)\n",
      "(280, 0.004673744551837444)\n",
      "(281, 0.004372600466012955)\n",
      "(282, 0.004090512171387672)\n",
      "(283, 0.003826307598501444)\n",
      "(284, 0.00357826123945415)\n",
      "(285, 0.0033459982369095087)\n",
      "(286, 0.003128095529973507)\n",
      "(287, 0.0029238422866910696)\n",
      "(288, 0.002732710912823677)\n",
      "(289, 0.0025531689170747995)\n",
      "(290, 0.002385201631113887)\n",
      "(291, 0.0022278449032455683)\n",
      "(292, 0.0020806423854082823)\n",
      "(293, 0.001942679169587791)\n",
      "(294, 0.0018134694546461105)\n",
      "(295, 0.001692615682259202)\n",
      "(296, 0.0015797539381310344)\n",
      "(297, 0.0014737098244950175)\n",
      "(298, 0.0013747481862083077)\n",
      "(299, 0.0012821848504245281)\n",
      "(300, 0.00119572295807302)\n",
      "(301, 0.0011146612232550979)\n",
      "(302, 0.0010390611132606864)\n",
      "(303, 0.0009683739044703543)\n",
      "(304, 0.000902304018381983)\n",
      "(305, 0.0008406001725234091)\n",
      "(306, 0.0007830369868315756)\n",
      "(307, 0.0007292027003131807)\n",
      "(308, 0.0006789079634472728)\n",
      "(309, 0.0006319958483800292)\n",
      "(310, 0.0005883213598281145)\n",
      "(311, 0.0005473752971738577)\n",
      "(312, 0.0005092526553198695)\n",
      "(313, 0.00047369449748657644)\n",
      "(314, 0.00044051374425180256)\n",
      "(315, 0.0004096127231605351)\n",
      "(316, 0.0003807522589340806)\n",
      "(317, 0.00035385205410420895)\n",
      "(318, 0.00032878611818887293)\n",
      "(319, 0.0003054472617805004)\n",
      "(320, 0.00028371860389597714)\n",
      "(321, 0.0002634477859828621)\n",
      "(322, 0.0002445841964799911)\n",
      "(323, 0.0002270267577841878)\n",
      "(324, 0.00021067696798127145)\n",
      "(325, 0.000195472122868523)\n",
      "(326, 0.00018131812976207584)\n",
      "(327, 0.00016815727576613426)\n",
      "(328, 0.00015590562543366104)\n",
      "(329, 0.00014452161849476397)\n",
      "(330, 0.0001339377195108682)\n",
      "(331, 0.00012409935879986733)\n",
      "(332, 0.00011496209481265396)\n",
      "(333, 0.00010646621376508847)\n",
      "(334, 9.858319390332326e-05)\n",
      "(335, 9.125508950091898e-05)\n",
      "(336, 8.446302672382444e-05)\n",
      "(337, 7.815525896148756e-05)\n",
      "(338, 7.229822949739173e-05)\n",
      "(339, 6.686293636448681e-05)\n",
      "(340, 6.182533252285793e-05)\n",
      "(341, 5.715520092053339e-05)\n",
      "(342, 5.2825474995188415e-05)\n",
      "(343, 4.8810710723046213e-05)\n",
      "(344, 4.509353675530292e-05)\n",
      "(345, 4.1641909774625674e-05)\n",
      "(346, 3.845277024083771e-05)\n",
      "(347, 3.5497672797646374e-05)\n",
      "(348, 3.2757696317275986e-05)\n",
      "(349, 3.0225566661101766e-05)\n",
      "(350, 2.7882415452040732e-05)\n",
      "(351, 2.5713085051393136e-05)\n",
      "(352, 2.370803485973738e-05)\n",
      "(353, 2.1852858480997384e-05)\n",
      "(354, 2.0138266336289234e-05)\n",
      "(355, 1.85525968845468e-05)\n",
      "(356, 1.7088905224227346e-05)\n",
      "(357, 1.5736743080196902e-05)\n",
      "(358, 1.4487801308860071e-05)\n",
      "(359, 1.3334651157492772e-05)\n",
      "(360, 1.2271030755073298e-05)\n",
      "(361, 1.1287462257314473e-05)\n",
      "(362, 1.0380429557699244e-05)\n",
      "(363, 9.544375643599778e-06)\n",
      "(364, 8.772807632340118e-06)\n",
      "(365, 8.062035703915171e-06)\n",
      "(366, 7.409401860059006e-06)\n",
      "(367, 6.803366431995528e-06)\n",
      "(368, 6.247431883821264e-06)\n",
      "(369, 5.734754267905373e-06)\n",
      "(370, 5.262965260044439e-06)\n",
      "(371, 4.828568762604846e-06)\n",
      "(372, 4.429989530763123e-06)\n",
      "(373, 4.062270818394609e-06)\n",
      "(374, 3.7247873478918336e-06)\n",
      "(375, 3.4135782698285766e-06)\n",
      "(376, 3.1280817438528175e-06)\n",
      "(377, 2.8656663744186517e-06)\n",
      "(378, 2.6244151740684174e-06)\n",
      "(379, 2.4030662189034047e-06)\n",
      "(380, 2.1996747818775475e-06)\n",
      "(381, 2.012758386626956e-06)\n",
      "(382, 1.8412495137454243e-06)\n",
      "(383, 1.684272774582496e-06)\n",
      "(384, 1.540036919323029e-06)\n",
      "(385, 1.4075881153985392e-06)\n",
      "(386, 1.2862810763181187e-06)\n",
      "(387, 1.175292709376663e-06)\n",
      "(388, 1.0732364899013191e-06)\n",
      "(389, 9.79848664428573e-07)\n",
      "(390, 8.943928833105019e-07)\n",
      "(391, 8.163849543052493e-07)\n",
      "(392, 7.447384291481285e-07)\n",
      "(393, 6.791576652176445e-07)\n",
      "(394, 6.191226020746399e-07)\n",
      "(395, 5.643487384077162e-07)\n",
      "(396, 5.143539851815149e-07)\n",
      "(397, 4.684655436903995e-07)\n",
      "(398, 4.2666221133913496e-07)\n",
      "(399, 3.885843398165889e-07)\n",
      "(400, 3.53654826312777e-07)\n",
      "(401, 3.2183038456423674e-07)\n",
      "(402, 2.9262355383252725e-07)\n",
      "(403, 2.66229562839726e-07)\n",
      "(404, 2.419498628114525e-07)\n",
      "(405, 2.1981698239414982e-07)\n",
      "(406, 1.997652816498885e-07)\n",
      "(407, 1.8150267067085224e-07)\n",
      "(408, 1.648223104666613e-07)\n",
      "(409, 1.4965280570322648e-07)\n",
      "(410, 1.3586110014784936e-07)\n",
      "(411, 1.2330912113611703e-07)\n",
      "(412, 1.1184327775026759e-07)\n",
      "(413, 1.0136429295926064e-07)\n",
      "(414, 9.199112582791713e-08)\n",
      "(415, 8.334463785786284e-08)\n",
      "(416, 7.556236880645884e-08)\n",
      "(417, 6.842967081865936e-08)\n",
      "(418, 6.200428970259964e-08)\n",
      "(419, 5.612537634647197e-08)\n",
      "(420, 5.081369991444262e-08)\n",
      "(421, 4.60111593270085e-08)\n",
      "(422, 4.159245747814566e-08)\n",
      "(423, 3.764578693221665e-08)\n",
      "(424, 3.4050380293138005e-08)\n",
      "(425, 3.0721547972234475e-08)\n",
      "(426, 2.7806105862282493e-08)\n",
      "(427, 2.511330876586726e-08)\n",
      "(428, 2.273504939864779e-08)\n",
      "(429, 2.0527188837604626e-08)\n",
      "(430, 1.856920128773254e-08)\n",
      "(431, 1.6737702779323627e-08)\n",
      "(432, 1.5112677331785562e-08)\n",
      "(433, 1.3640925722313568e-08)\n",
      "(434, 1.2321747178134501e-08)\n",
      "(435, 1.1162138768838759e-08)\n",
      "(436, 1.005070515702755e-08)\n",
      "(437, 9.055702854254832e-09)\n",
      "(438, 8.175229382345606e-09)\n",
      "(439, 7.3915500387045086e-09)\n",
      "(440, 6.700548116356231e-09)\n",
      "(441, 6.053346268686255e-09)\n",
      "(442, 5.458911545019873e-09)\n",
      "(443, 4.943380815802811e-09)\n",
      "(444, 4.482814119910472e-09)\n",
      "(445, 4.0296557202168515e-09)\n",
      "(446, 3.6483023269084924e-09)\n",
      "(447, 3.3212448347086365e-09)\n",
      "(448, 3.001940696023553e-09)\n",
      "(449, 2.7262314628018203e-09)\n",
      "(450, 2.486694183900795e-09)\n",
      "(451, 2.252350750353571e-09)\n",
      "(452, 2.0457415761399034e-09)\n",
      "(453, 1.8549778380005932e-09)\n",
      "(454, 1.697304075065631e-09)\n",
      "(455, 1.5428840427489376e-09)\n",
      "(456, 1.4142279569639982e-09)\n",
      "(457, 1.2967624751780704e-09)\n",
      "(458, 1.1881832184812424e-09)\n",
      "(459, 1.0896088475931265e-09)\n",
      "(460, 9.965724911964458e-10)\n",
      "(461, 9.160295300070231e-10)\n",
      "(462, 8.392412875757316e-10)\n",
      "(463, 7.776290167349487e-10)\n",
      "(464, 7.236456989190287e-10)\n",
      "(465, 6.638627425559207e-10)\n",
      "(466, 6.140306041402255e-10)\n",
      "(467, 5.691935256457725e-10)\n",
      "(468, 5.393148705401529e-10)\n",
      "(469, 4.995531766027739e-10)\n",
      "(470, 4.674898246292969e-10)\n",
      "(471, 4.411415677196828e-10)\n",
      "(472, 4.1001108064264713e-10)\n",
      "(473, 3.9072883817325987e-10)\n",
      "(474, 3.6234992784045517e-10)\n",
      "(475, 3.4154207240177925e-10)\n",
      "(476, 3.2432090346645737e-10)\n",
      "(477, 3.0516644766720447e-10)\n",
      "(478, 2.9551150415585425e-10)\n",
      "(479, 2.7796315249517534e-10)\n",
      "(480, 2.6173274658702894e-10)\n",
      "(481, 2.518946717877668e-10)\n",
      "(482, 2.410803778829518e-10)\n",
      "(483, 2.288951667095418e-10)\n",
      "(484, 2.1691792806421972e-10)\n",
      "(485, 2.072245847140053e-10)\n",
      "(486, 2.0121566912667532e-10)\n",
      "(487, 1.9007417861871545e-10)\n",
      "(488, 1.8497410547713145e-10)\n",
      "(489, 1.7488105408247634e-10)\n",
      "(490, 1.7012370678859412e-10)\n",
      "(491, 1.6032995764359015e-10)\n",
      "(492, 1.5835339983727437e-10)\n",
      "(493, 1.5392365160238342e-10)\n",
      "(494, 1.4859349861673365e-10)\n",
      "(495, 1.4299986195176473e-10)\n",
      "(496, 1.4024179040283968e-10)\n",
      "(497, 1.3414680477552565e-10)\n",
      "(498, 1.303032820532124e-10)\n",
      "(499, 1.2722195519287993e-10)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables.\n",
    "x = Variable(torch.randn(N, D_in))\n",
    "y = Variable(torch.randn(N, D_out), requires_grad=False)\n",
    "\n",
    "# Use the nn package to define our model and loss function.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(size_average=False)\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of\n",
    "# the model for us. Here we will use Adam; the optim package contains many other\n",
    "# optimization algoriths. The first argument to the Adam constructor tells the\n",
    "# optimizer which Variables it should update.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y by passing x to the model.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # Compute and print loss.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    print(t, loss.data[0])\n",
    "\n",
    "    # Before the backward pass, use the optimizer object to zero all of the\n",
    "    # gradients for the variables it will update (which are the learnable weights\n",
    "    # of the model)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Backward pass: compute gradient of the loss with respect to model\n",
    "    # parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Calling the step function on an Optimizer makes an update to its\n",
    "    # parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ExecutableNotFound",
     "evalue": "failed to execute ['dot', '-Tpdf', '-O', 'Digraph.gv'], make sure the Graphviz executables are on your systems' PATH",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mExecutableNotFound\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-127-601aaba565a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Fischer/anaconda/lib/python2.7/site-packages/graphviz/files.pyc\u001b[0m in \u001b[0;36mview\u001b[0;34m(self, filename, directory, cleanup)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \"\"\"\n\u001b[1;32m    197\u001b[0m         return self.render(filename=filename, directory=directory, view=True,\n\u001b[0;32m--> 198\u001b[0;31m                            cleanup=cleanup)\n\u001b[0m\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_view\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Fischer/anaconda/lib/python2.7/site-packages/graphviz/files.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, filename, directory, view, cleanup)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0mrendered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleanup\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Fischer/anaconda/lib/python2.7/site-packages/graphviz/backend.pyc\u001b[0m in \u001b[0;36mrender\u001b[0;34m(engine, format, filepath, quiet)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mExecutableNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mExecutableNotFound\u001b[0m: failed to execute ['dot', '-Tpdf', '-O', 'Digraph.gv'], make sure the Graphviz executables are on your systems' PATH"
     ]
    }
   ],
   "source": [
    "\n",
    "from graphviz import Digraph\n",
    "import re\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "\n",
    "def make_dot(var):\n",
    "    node_attr = dict(style='filled',\n",
    "                     shape='box',\n",
    "                     align='left',\n",
    "                     fontsize='12',\n",
    "                     ranksep='0.1',\n",
    "                     height='0.2')\n",
    "    dot = Digraph(node_attr=node_attr, graph_attr=dict(size=\"12,12\"))\n",
    "    seen = set()\n",
    "\n",
    "    def add_nodes(var):\n",
    "        if var not in seen:\n",
    "            if isinstance(var, Variable):\n",
    "                value = '('+(', ').join(['%d'% v for v in var.size()])+')'\n",
    "                dot.node(str(id(var)), str(value), fillcolor='lightblue')\n",
    "            else:\n",
    "                dot.node(str(id(var)), str(type(var).__name__))\n",
    "            seen.add(var)\n",
    "            if hasattr(var, 'previous_functions'):\n",
    "                for u in var.previous_functions:\n",
    "                    dot.edge(str(id(u[0])), str(id(var)))\n",
    "                    add_nodes(u[0])\n",
    "    add_nodes(var.creator)\n",
    "    return dot\n",
    "\n",
    "\n",
    "inputs = torch.randn(1,3,224,224)\n",
    "resnet18 = models.resnet18()\n",
    "y = resnet18(Variable(inputs))\n",
    "# print(y)\n",
    "x = Variable(torch.randn(1,4,33,33), requires_grad = True)\n",
    "y_pred = net.forward(x)\n",
    "\n",
    "g = make_dot(y_pred)\n",
    "g.view()\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
